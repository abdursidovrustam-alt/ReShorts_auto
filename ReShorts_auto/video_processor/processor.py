#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –º–æ–¥—É–ª—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ —É–Ω–∏–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–∏–¥–µ–æ
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –ª—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ –∏–∑ —Ç–æ–ø–æ–≤—ã—Ö GitHub –ø—Ä–æ–µ–∫—Ç–æ–≤

–ê–≤—Ç–æ—Ä: MiniMax Agent
–î–∞—Ç–∞: 2025-10-17
–í–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω: ShortGPT, auto-yt-shorts, short-video-maker
"""

import os
import sys
import cv2
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple, Callable
from datetime import datetime
import random
import json
import logging
from dataclasses import dataclass, asdict
import tempfile
import shutil
import subprocess

# –í–∏–¥–µ–æ –æ–±—Ä–∞–±–æ—Ç–∫–∞
try:
    # –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ –¥–ª—è –Ω–æ–≤–æ–π –≤–µ—Ä—Å–∏–∏ moviepy (2.x)
    from moviepy import (
        VideoFileClip, AudioFileClip, CompositeVideoClip, 
        CompositeAudioClip, TextClip, ColorClip, 
        concatenate_videoclips, vfx, afx
    )
except ImportError:
    # –î–ª—è —Å—Ç–∞—Ä–æ–π –≤–µ—Ä—Å–∏–∏ moviepy (1.x)
    from moviepy.editor import (
        VideoFileClip, AudioFileClip, CompositeVideoClip, 
        CompositeAudioClip, TextClip, ColorClip, 
        concatenate_videoclips, vfx, afx
    )

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
from PIL import Image, ImageFilter, ImageEnhance, ImageDraw, ImageFont
from scipy import ndimage

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class ProcessingResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ"""
    success: bool
    input_file: str
    output_file: Optional[str] = None
    duration: float = 0.0
    size_mb: float = 0.0
    resolution: str = ""
    applied_effects: List[str] = None
    uniqueness_score: float = 0.0
    processing_time: float = 0.0
    error: Optional[str] = None
    metadata: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.applied_effects is None:
            self.applied_effects = []
        if self.metadata is None:
            self.metadata = {}
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


class AdvancedVideoProcessor:
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –≤–∏–¥–µ–æ —Å 25+ —ç—Ñ—Ñ–µ–∫—Ç–∞–º–∏ –∏ —É–Ω–∏–∫–∞–ª–∏–∑–∞—Ü–∏–µ–π
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        
        Args:
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        """
        self.config = config or {}
        
        # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        self.output_dir = Path(self.config.get('output_dir', 'processed'))
        self.temp_dir = Path(self.config.get('temp_dir', 'tmp/video_processing'))
        self.cache_dir = Path(self.config.get('cache_dir', 'tmp/cache'))
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
        for directory in [self.output_dir, self.temp_dir, self.cache_dir]:
            directory.mkdir(parents=True, exist_ok=True)
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.default_quality = self.config.get('quality', '720p')
        self.max_duration = self.config.get('max_duration', 60)
        self.target_fps = self.config.get('target_fps', 30)
        self.clear_metadata = self.config.get('clear_metadata', True)
        
        # –ö–∞—á–µ—Å—Ç–≤–æ –≤–∏–¥–µ–æ
        self.quality_presets = {
            '360p': {'height': 360, 'bitrate': '800k', 'audio_bitrate': '96k'},
            '480p': {'height': 480, 'bitrate': '1200k', 'audio_bitrate': '128k'},
            '720p': {'height': 720, 'bitrate': '2500k', 'audio_bitrate': '192k'},
            '1080p': {'height': 1080, 'bitrate': '5000k', 'audio_bitrate': '256k'},
        }
        
        # –î–æ—Å—Ç—É–ø–Ω—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã
        self.video_effects = {
            # –¶–≤–µ—Ç–æ–≤—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã
            'brightness': self._effect_brightness,
            'contrast': self._effect_contrast,
            'saturation': self._effect_saturation,
            'hue': self._effect_hue,
            'temperature': self._effect_temperature,
            
            # –°—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —ç—Ñ—Ñ–µ–∫—Ç—ã
            'sepia': self._effect_sepia,
            'grayscale': self._effect_grayscale,
            'vintage': self._effect_vintage,
            'cinematic': self._effect_cinematic,
            'neon': self._effect_neon,
            'retro': self._effect_retro,
            'cyberpunk': self._effect_cyberpunk,
            
            # –§–∏–ª—å—Ç—Ä—ã
            'blur': self._effect_blur,
            'sharpen': self._effect_sharpen,
            'edge_enhance': self._effect_edge_enhance,
            'emboss': self._effect_emboss,
            'noise': self._effect_noise,
            'film_grain': self._effect_film_grain,
            
            # –ì–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ —ç—Ñ—Ñ–µ–∫—Ç—ã
            'vignette': self._effect_vignette,
            'fisheye': self._effect_fisheye,
            'chromatic_aberration': self._effect_chromatic_aberration,
            
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã
            'glitch': self._effect_glitch,
            'pixelate': self._effect_pixelate,
            'ascii': self._effect_ascii,
            'mirror_horizontal': self._effect_mirror_horizontal,
            'mirror_vertical': self._effect_mirror_vertical,
        }
        
        logger.info(f"üé¨ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω AdvancedVideoProcessor —Å {len(self.video_effects)} —ç—Ñ—Ñ–µ–∫—Ç–∞–º–∏")
    
    def process_video(
        self,
        input_file: str,
        effects: List[str] = None,
        options: Dict[str, Any] = None,
        output_file: str = None
    ) -> ProcessingResult:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º —ç—Ñ—Ñ–µ–∫—Ç–æ–≤
        
        Args:
            input_file: –ü—É—Ç—å –∫ –≤—Ö–æ–¥–Ω–æ–º—É –≤–∏–¥–µ–æ
            effects: –°–ø–∏—Å–æ–∫ —ç—Ñ—Ñ–µ–∫—Ç–æ–≤ –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è
            options: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ–ø—Ü–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
            output_file: –ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        
        Returns:
            ProcessingResult —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        start_time = datetime.now()
        input_path = Path(input_file)
        
        if not input_path.exists():
            return ProcessingResult(
                success=False,
                input_file=input_file,
                error=f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {input_file}"
            )
        
        logger.info(f"üìπ –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {input_path.name}")
        
        try:
            options = options or {}
            effects = effects or self._select_random_effects()
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–º–µ–Ω–∏ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            if output_file is None:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                output_file = self.output_dir / f"processed_{timestamp}_{input_path.stem}.mp4"
            else:
                output_file = Path(output_file)
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –≤–∏–¥–µ–æ
            with VideoFileClip(str(input_file)) as video:
                original_duration = video.duration
                original_size = video.size
                
                # –û–±—Ä–µ–∑–∫–∞ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
                if video.duration > self.max_duration:
                    logger.info(f"‚è±Ô∏è –û–±—Ä–µ–∑–∫–∞ –≤–∏–¥–µ–æ: {video.duration:.1f}—Å ‚Üí {self.max_duration}—Å")
                    video = video.subclip(0, self.max_duration)
                
                # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–æ–≤
                processed_video = self._apply_effects_chain(video, effects, options)
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞—É–¥–∏–æ
                processed_video = self._process_audio(processed_video, options)
                
                # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å—É–±—Ç–∏—Ç—Ä–æ–≤/—Ç–µ–∫—Å—Ç–∞
                if options.get('add_text'):
                    processed_video = self._add_text_overlay(processed_video, options)
                
                # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
                processed_video = self._optimize_video(processed_video, options)
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
                quality = options.get('quality', self.default_quality)
                preset = self.quality_presets.get(quality, self.quality_presets['720p'])
                
                logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: {output_file.name}")
                processed_video.write_videofile(
                    str(output_file),
                    codec='libx264',
                    audio_codec='aac',
                    bitrate=preset['bitrate'],
                    audio_bitrate=preset['audio_bitrate'],
                    fps=self.target_fps,
                    preset='medium',
                    threads=4,
                    verbose=False,
                    logger=None
                )
            
            # –û—á–∏—Å—Ç–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            if self.clear_metadata:
                self._clear_file_metadata(output_file)
            
            # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
            processing_time = (datetime.now() - start_time).total_seconds()
            file_size_mb = output_file.stat().st_size / (1024 * 1024)
            uniqueness_score = self._calculate_uniqueness_score(effects, options)
            
            result = ProcessingResult(
                success=True,
                input_file=str(input_file),
                output_file=str(output_file),
                duration=original_duration,
                size_mb=round(file_size_mb, 2),
                resolution=f"{original_size[0]}x{original_size[1]}",
                applied_effects=effects,
                uniqueness_score=round(uniqueness_score, 3),
                processing_time=round(processing_time, 2),
                metadata={
                    'original_duration': original_duration,
                    'final_duration': min(original_duration, self.max_duration),
                    'quality': quality,
                    'fps': self.target_fps
                }
            )
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –æ–±—Ä–∞–±–æ—Ç–∫–µ
            self._save_processing_info(result)
            
            logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {processing_time:.2f}—Å")
            logger.info(f"üìä –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å: {uniqueness_score:.1%}")
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}", exc_info=True)
            return ProcessingResult(
                success=False,
                input_file=str(input_file),
                error=str(e),
                processing_time=(datetime.now() - start_time).total_seconds()
            )
    
    def batch_process(
        self,
        input_files: List[str],
        effects: List[str] = None,
        options: Dict[str, Any] = None,
        progress_callback: Callable[[int, int], None] = None
    ) -> List[ProcessingResult]:
        """
        –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ
        
        Args:
            input_files: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ –≤—Ö–æ–¥–Ω—ã–º –≤–∏–¥–µ–æ
            effects: –≠—Ñ—Ñ–µ–∫—Ç—ã –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è
            options: –û–ø—Ü–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
            progress_callback: –§—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –≤—ã–∑–æ–≤–∞ –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        logger.info(f"üì¶ –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ {len(input_files)} –≤–∏–¥–µ–æ")
        
        results = []
        for i, input_file in enumerate(input_files, 1):
            if progress_callback:
                progress_callback(i, len(input_files))
            
            logger.info(f"[{i}/{len(input_files)}] –û–±—Ä–∞–±–æ—Ç–∫–∞: {Path(input_file).name}")
            result = self.process_video(input_file, effects, options)
            results.append(result)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        successful = sum(1 for r in results if r.success)
        total_time = sum(r.processing_time for r in results)
        
        logger.info(f"‚úÖ –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
        logger.info(f"   –£—Å–ø–µ—à–Ω–æ: {successful}/{len(input_files)}")
        logger.info(f"   –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.2f}—Å")
        
        return results
    
    def _apply_effects_chain(
        self,
        video: VideoFileClip,
        effects: List[str],
        options: Dict[str, Any]
    ) -> VideoFileClip:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–ø–æ—á–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–æ–≤"""
        processed = video
        
        for effect_name in effects:
            if effect_name in self.video_effects:
                try:
                    logger.info(f"   üé® –ü—Ä–∏–º–µ–Ω—è—é —ç—Ñ—Ñ–µ–∫—Ç: {effect_name}")
                    effect_func = self.video_effects[effect_name]
                    processed = effect_func(processed, options)
                except Exception as e:
                    logger.warning(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ —ç—Ñ—Ñ–µ–∫—Ç–∞ {effect_name}: {e}")
            else:
                logger.warning(f"   ‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç: {effect_name}")
        
        return processed
    
    # ===========================================
    # –¶–í–ï–¢–û–í–´–ï –≠–§–§–ï–ö–¢–´
    # ===========================================
    
    def _effect_brightness(self, video: VideoFileClip, options: Dict) -> VideoFileClip:
        """–ò–∑–º–µ–Ω–µ–Ω–∏–µ —è—Ä–∫–æ—Å—Ç–∏"""
        factor = options.get('brightness_factor', random.uniform(0.85, 1.15))
        
        def adjust(frame):
            return np.clip(frame * factor, 0, 255).astype(np.uint8)
        
        return video.fl_image(adjust)
    
    def _effect_contrast(self, video: VideoFileClip, options: Dict) -> VideoFileClip:
        """–ò–∑–º–µ–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç–∏"""
        factor = options.get('contrast_factor', random.uniform(0.9, 1.3))
        
        def adjust(frame):
            mean = np.mean(frame)
            return np.clip((frame - mean) * factor + mean, 0, 255).astype(np.uint8)
        
        return video.fl_image(adjust)
    
    def _effect_saturation(self, video: VideoFileClip, options: Dict) -> VideoFileClip:
        """–ò–∑–º–µ–Ω–µ–Ω–∏–µ –Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç–∏"""
        factor = options.get('saturation_factor', random.uniform(0.8, 1.4))
        
        def adjust(frame):
            hsv = cv2.cvtColor(frame, cv2.COLOR_RGB2HSV).astype(np.float32)
            hsv[:, :, 1] *= factor
            hsv[:, :, 1] = np.clip(hsv[:, :, 1], 0, 255)
            return cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2RGB)
        
        return video.fl_image(adjust)
    
    def _effect_hue(self, video: VideoFileClip, options: Dict) -> VideoFileClip:
        """–°–º–µ—â–µ–Ω–∏–µ –æ—Ç—Ç–µ–Ω–∫–∞"""
        shift = options.get('hue_shift', random.randint(-30, 30))
        
        def adjust(frame):
            hsv = cv2.cvtColor(frame, cv2.COLOR_RGB2HSV).astype(np.int16)
            hsv[:, :, 0] = (hsv[:, :, 0] + shift) % 180
            return cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2RGB)
        
        return video.fl_image(adjust)
    
    def _effect_temperature(self, video: VideoFileClip, options: Dict) -> VideoFileClip:
        """–¶–≤–µ—Ç–æ–≤–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ (—Ç–µ–ø–ª–µ–µ/—Ö–æ–ª–æ–¥–Ω–µ–µ)"""
        temp = options.get('temperature', random.uniform(-0.1, 0.1))
        
        def adjust(frame):
            adjusted = frame.copy().astype(np.float32)
            if temp > 0:  # –¢–µ–ø–ª–µ–µ
                adjusted[:, :, 0] *= (1 + temp)  # –ë–æ–ª—å—à–µ –∫—Ä–∞—Å–Ω–æ–≥–æ
                adjusted[:, :, 2] *= (1 - temp * 0.5)  # –ú–µ–Ω—å—à–µ —Å–∏–Ω–µ–≥–æ
            else:  # –•–æ–ª–æ–¥–Ω–µ–µ
                adjusted[:, :, 0] *= (1 + temp)  # –ú–µ–Ω—å—à–µ –∫—Ä–∞—Å–Ω–æ–≥–æ
                adjusted[:, :, 2] *= (1 - temp)  # –ë–æ–ª—å—à–µ —Å–∏–Ω–µ–≥–æ
            return np.clip(adjusted, 0, 255).astype(np.uint8)
        
        return video.fl_image(adjust)
    
    # ===========================================
    # –°–¢–ò–õ–ò–°–¢–ò–ß–ï–°–ö–ò–ï –≠–§–§–ï–ö–¢–´
    # ===========================================
    
    def _effect_sepia(self, video: VideoFileClip, options: Dict) -> VideoFileClip:
        """–°–µ–ø–∏—è —Ñ–∏–ª—å—Ç—Ä"""
        def apply(frame):
            kernel = np.array([
                [0.393, 0.769, 0.189],
                [0.349, 0.686, 0.168],
                [0.272, 0.534, 0.131]
            ])
            return np.clip(frame @ kernel.T, 0, 255).astype(np.uint8)
        
        return video.fl_image(apply)
    
    def _effect_grayscale(self, video: VideoFileClip, options: Dict) -> VideoFileClip:
        """–ß–µ—Ä–Ω–æ-–±–µ–ª—ã–π"""
        def apply(frame):
            gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)
            return cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)
        
        return video.fl_image(apply)
    
    def _effect_vintage(self, video: VideoFileClip, options: Dict) -> VideoFileClip:
        """–í–∏–Ω—Ç–∞–∂–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç"""
        def apply(frame):
            # –°–µ–ø–∏—è
            kernel = np.array([[0.393, 0.769, 0.189],
                             [0.349, 0.686, 0.168],
                             [0.272, 0.534, 0.131]])
            sepia = np.clip(frame @ kernel.T, 0, 255)
            # –°–Ω–∏–∂–∞–µ–º –∫–æ–Ω—Ç—Ä–∞—Å—Ç –∏ –¥–æ–±–∞–≤–ª—è–µ–º –±–ª–µ–∫–ª–æ—Å—Ç–∏
            vintage = sepia * 0.85 + 30
            return np.clip(vintage, 0, 255).astype(np.uint8)
        
        return video.fl_image(apply)
    
    def _effect_cinematic(self, video: VideoFileClip, options: Dict) -> VideoFileClip:
        """–ö–∏–Ω–µ–º–∞—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –≤–∏–¥ (—à–∏—Ä–æ–∫–æ—Ñ–æ—Ä–º–∞—Ç–Ω—ã–π)"""
        def apply(frame):
            # –¢–µ–º–Ω–µ–µ –≤ —Ç–µ–Ω—è—Ö, –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–µ–µ
            adjusted = frame.astype(np.float32)
            adjusted = (adjusted - 128) * 1.15 + 128
            adjusted *= 0.95
            return np.clip(adjusted, 0, 255).astype(np.uint8)
        
        return video.fl_image(apply)
    
    def _effect_neon(self, video: VideoFileClip, options: Dict) -> VideoFileClip:
        """–ù–µ–æ–Ω–æ–≤—ã–π —ç—Ñ—Ñ–µ–∫—Ç"""
        def apply(frame):
            # –ü–æ–≤—ã—à–∞–µ–º –Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç—å –∏ —è—Ä–∫–æ—Å—Ç—å
            hsv = cv2.cvtColor(frame, cv2.COLOR_RGB2HSV).astype(np.float32)
            hsv[:, :, 1] *= 1.6  # –ù–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç—å
            hsv[:, :, 2] *= 1.2  # –Ø—Ä–∫–æ—Å—Ç—å
            hsv = np.clip(hsv, 0, 255)
            neon = cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2RGB)
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–≤–µ—á–µ–Ω–∏–µ
            glow = cv2.GaussianBlur(neon, (21, 21), 0)
            return cv2.addWeighted(neon, 0.7, glow, 0.3, 0)
        
        return video.fl_image(apply)
    
    def _effect_retro(self, video: VideoFileClip, options: Dict) -> VideoFileClip:
        """–†–µ—Ç—Ä–æ —ç—Ñ—Ñ–µ–∫—Ç 80-—Ö"""
        def apply(frame):
            # –°–Ω–∏–∂–∞–µ–º –Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç—å
            hsv = cv2.cvtColor(frame, cv2.COLOR_RGB2HSV).astype(np.float32)
            hsv[:, :, 1] *= 0.7
            retro = cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2RGB)
            # –¢–µ–ø–ª—ã–µ —Ç–æ–Ω–∞
            retro = retro.astype(np.float32)
            retro[:, :, 0] *= 1.1  # –ö—Ä–∞—Å–Ω—ã–π
            retro[:, :, 2] *= 0.9  # –°–∏–Ω–∏–π
            return np.clip(retro, 0, 255).astype(np.uint8)
        
        return video.fl_image(apply)
    
    def _effect_cyberpunk(self, video: VideoFileClip, options: Dict) -> VideoFileClip:
        """–ö–∏–±–µ—Ä–ø–∞–Ω–∫ —Å—Ç–∏–ª—å"""
        def apply(frame):
            # –£—Å–∏–ª–∏–≤–∞–µ–º —Å–∏–Ω–∏–π –∏ —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π
            adjusted = frame.astype(np.float32)
            adjusted[:, :, 0] *= 1.1  # –ö—Ä–∞—Å–Ω—ã–π
            adjusted[:, :, 1] *= 0.9  # –ó–µ–ª–µ–Ω—ã–π
            adjusted[:, :, 2] *= 1.3  # –°–∏–Ω–∏–π
            # –ü–æ–≤—ã—à–∞–µ–º –∫–æ–Ω—Ç—Ä–∞—Å—Ç
            adjusted = (adjusted - 128) * 1.2 + 128
            return np.clip(adjusted, 0, 255).astype(np.uint8)
        
        return video.fl_image(apply)
    
    # ===========================================
    # –§–ò–õ–¨–¢–†–´
    # ===========================================
    
    def _effect_blur(self, video: VideoFileClip, options: Dict) -> VideoFileClip:
        """–†–∞–∑–º—ã—Ç–∏–µ"""
        strength = options.get('blur_strength', random.uniform(1.0, 3.0))
        kernel_size = int(strength * 2) * 2 + 1
        
        def apply(frame):
            return cv2.GaussianBlur(frame, (kernel_size, kernel_size), strength)
        
        return video.fl_image(apply)
    
    def _effect_sharpen(self, video: VideoFileClip, options: Dict) -> VideoFileClip:
        """–ü–æ–≤—ã—à–µ–Ω–∏–µ —Ä–µ–∑–∫–æ—Å—Ç–∏"""
        strength = options.get('sharpen_strength', random.uniform(0.5, 1.5))
        
        def apply(frame):
            kernel = np.array([[-1, -1, -1],
                             [-1, 9, -1],
                             [-1, -1, -1]]) * (strength / 3)
            return cv2.filter2D(frame, -1, kernel)
        
        return video.fl_image(apply)
    
    def _effect_edge_enhance(self, video: VideoFileClip, options: Dict) -> VideoFileClip:
        """–£—Å–∏–ª–µ–Ω–∏–µ –∫—Ä–∞–µ–≤"""
        def apply(frame):
            kernel = np.array([[0, -1, 0],
                             [-1, 5, -1],
                             [0, -1, 0]])
            return cv2.filter2D(frame, -1, kernel)
        
        return video.fl_image(apply)
    
    def _effect_emboss(self, video: VideoFileClip, options: Dict) -> VideoFileClip:
        """–≠—Ñ—Ñ–µ–∫—Ç —Ç–∏—Å–Ω–µ–Ω–∏—è"""
        def apply(frame):
            kernel = np.array([[-2, -1, 0],
                             [-1, 1, 1],
                             [0, 1, 2]])
            embossed = cv2.filter2D(frame, -1, kernel)
            return np.clip(embossed + 128, 0, 255).astype(np.uint8)
        
        return video.fl_image(apply)
    
    def _effect_noise(self, video: VideoFileClip, options: Dict) -> VideoFileClip:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —à—É–º–∞"""
        intensity = options.get('noise_intensity', random.uniform(0.05, 0.15))
        
        def apply(frame):
            noise = np.random.normal(0, intensity * 255, frame.shape)
            return np.clip(frame + noise, 0, 255).astype(np.uint8)
        
        return video.fl_image(apply)
    
    def _effect_film_grain(self, video: VideoFileClip, options: Dict) -> VideoFileClip:
        """–ü–ª–µ–Ω–æ—á–Ω–æ–µ –∑–µ—Ä–Ω–æ"""
        intensity = options.get('grain_intensity', random.uniform(0.1, 0.25))
        
        def apply(frame):
            grain = np.random.normal(0, intensity * 255, frame.shape[:2])
            grain = np.stack([grain] * 3, axis=2)
            return np.clip(frame + grain, 0, 255).astype(np.uint8)
        
        return video.fl_image(apply)
    
    # ===========================================
    # –ì–ï–û–ú–ï–¢–†–ò–ß–ï–°–ö–ò–ï –≠–§–§–ï–ö–¢–´
    # ===========================================
    
    def _effect_vignette(self, video: VideoFileClip, options: Dict) -> VideoFileClip:
        """–í–∏–Ω—å–µ—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ"""
        strength = options.get('vignette_strength', random.uniform(0.4, 0.7))
        
        def apply(frame):
            h, w = frame.shape[:2]
            X, Y = np.meshgrid(np.arange(w), np.arange(h))
            center_x, center_y = w // 2, h // 2
            max_dist = np.sqrt(center_x**2 + center_y**2)
            distance = np.sqrt((X - center_x)**2 + (Y - center_y)**2)
            vignette = 1 - (distance / max_dist) * strength
            vignette = np.clip(vignette, 0, 1)[:, :, np.newaxis]
            return (frame * vignette).astype(np.uint8)
        
        return video.fl_image(apply)
    
    def _effect_fisheye(self, video: VideoFileClip, options: Dict) -> VideoFileClip:
        """–≠—Ñ—Ñ–µ–∫—Ç —Ä—ã–±—å–µ–≥–æ –≥–ª–∞–∑–∞"""
        strength = options.get('fisheye_strength', random.uniform(0.2, 0.4))
        
        def apply(frame):
            h, w = frame.shape[:2]
            # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            center_x, center_y = w // 2, h // 2
            
            map_x = np.zeros((h, w), dtype=np.float32)
            map_y = np.zeros((h, w), dtype=np.float32)
            
            for y in range(h):
                for x in range(w):
                    dx = (x - center_x) / center_x
                    dy = (y - center_y) / center_y
                    r = np.sqrt(dx**2 + dy**2)
                    
                    if r < 1:
                        r_new = r + strength * r**3
                        factor = r_new / max(r, 0.001)
                        map_x[y, x] = center_x + dx * center_x * factor
                        map_y[y, x] = center_y + dy * center_y * factor
                    else:
                        map_x[y, x] = x
                        map_y[y, x] = y
            
            return cv2.remap(frame, map_x, map_y, cv2.INTER_LINEAR)
        
        return video.fl_image(apply)
    
    def _effect_chromatic_aberration(self, video: VideoFileClip, options: Dict) -> VideoFileClip:
        """–•—Ä–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∞–±–µ—Ä—Ä–∞—Ü–∏—è"""
        offset = options.get('aberration_offset', random.randint(2, 5))
        
        def apply(frame):
            result = frame.copy()
            # –°–¥–≤–∏–≥–∞–µ–º –∫—Ä–∞—Å–Ω—ã–π –∫–∞–Ω–∞–ª
            result[:, :, 0] = np.roll(frame[:, :, 0], offset, axis=1)
            # –°–¥–≤–∏–≥–∞–µ–º —Å–∏–Ω–∏–π –∫–∞–Ω–∞–ª –≤ –¥—Ä—É–≥—É—é —Å—Ç–æ—Ä–æ–Ω—É
            result[:, :, 2] = np.roll(frame[:, :, 2], -offset, axis=1)
            return result
        
        return video.fl_image(apply)
    
    # ===========================================
    # –°–ü–ï–¶–ò–ê–õ–¨–ù–´–ï –≠–§–§–ï–ö–¢–´
    # ===========================================
    
    def _effect_glitch(self, video: VideoFileClip, options: Dict) -> VideoFileClip:
        """–ì–ª–∏—Ç—á —ç—Ñ—Ñ–µ–∫—Ç"""
        intensity = options.get('glitch_intensity', random.uniform(0.15, 0.35))
        
        def apply(frame):
            result = frame.copy()
            h = frame.shape[0]
            
            # –°–ª—É—á–∞–π–Ω—ã–µ —Å–¥–≤–∏–≥–∏ –ø–æ –∫–∞–Ω–∞–ª–∞–º
            if random.random() < intensity:
                shift = random.randint(-8, 8)
                result[:, :, 0] = np.roll(result[:, :, 0], shift, axis=1)
            
            if random.random() < intensity:
                shift = random.randint(-5, 5)
                result[:, :, 1] = np.roll(result[:, :, 1], shift, axis=0)
            
            # –°–ª—É—á–∞–π–Ω—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã–µ –ø–æ–ª–æ—Å—ã
            if random.random() < intensity / 2:
                y_start = random.randint(0, h - 20)
                y_end = y_start + random.randint(3, 15)
                result[y_start:y_end, :] = np.roll(
                    result[y_start:y_end, :],
                    random.randint(-30, 30),
                    axis=1
                )
            
            return result
        
        return video.fl_image(apply)
    
    def _effect_pixelate(self, video: VideoFileClip, options: Dict) -> VideoFileClip:
        """–ü–∏–∫—Å–µ–ª–∏–∑–∞—Ü–∏—è"""
        pixel_size = options.get('pixel_size', random.randint(8, 20))
        
        def apply(frame):
            h, w = frame.shape[:2]
            # –£–º–µ–Ω—å—à–∞–µ–º
            small = cv2.resize(frame, (w // pixel_size, h // pixel_size),
                             interpolation=cv2.INTER_LINEAR)
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –æ–±—Ä–∞—Ç–Ω–æ
            return cv2.resize(small, (w, h), interpolation=cv2.INTER_NEAREST)
        
        return video.fl_image(apply)
    
    def _effect_ascii(self, video: VideoFileClip, options: Dict) -> VideoFileClip:
        """ASCII art —ç—Ñ—Ñ–µ–∫—Ç (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π)"""
        levels = " .:-=+*#%@"
        
        def apply(frame):
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –≥—Ä–∞–¥–∞—Ü–∏–∏ —Å–µ—Ä–æ–≥–æ
            gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)
            # –£–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ
            small = cv2.resize(gray, (frame.shape[1] // 4, frame.shape[0] // 4))
            # –ö–≤–∞–Ω—Ç—É–µ–º —è—Ä–∫–æ—Å—Ç—å
            quantized = (small / 255 * (len(levels) - 1)).astype(int)
            quantized = (quantized / (len(levels) - 1) * 255).astype(np.uint8)
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤ –∏—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä
            result = cv2.resize(quantized, (frame.shape[1], frame.shape[0]),
                              interpolation=cv2.INTER_NEAREST)
            return cv2.cvtColor(result, cv2.COLOR_GRAY2RGB)
        
        return video.fl_image(apply)
    
    def _effect_mirror_horizontal(self, video: VideoFileClip, options: Dict) -> VideoFileClip:
        """–ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–æ–µ –∑–µ—Ä–∫–∞–ª–æ"""
        try:
            return video.fx(vfx.mirror_x)
        except:
            # Fallback: —Ä—É—á–Ω–æ–µ –∑–µ—Ä–∫–∞–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
            return video.fl_image(lambda frame: np.fliplr(frame))
    
    def _effect_mirror_vertical(self, video: VideoFileClip, options: Dict) -> VideoFileClip:
        """–í–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–æ–µ –∑–µ—Ä–∫–∞–ª–æ"""
        try:
            return video.fx(vfx.mirror_y)
        except:
            # Fallback: —Ä—É—á–Ω–æ–µ –∑–µ—Ä–∫–∞–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
            return video.fl_image(lambda frame: np.flipud(frame))
    
    # ===========================================
    # –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –ú–ï–¢–û–î–´
    # ===========================================
    
    def _process_audio(self, video: VideoFileClip, options: Dict) -> VideoFileClip:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∞—É–¥–∏–æ"""
        if not video.audio:
            return video
        
        try:
            # –ò–∑–º–µ–Ω–µ–Ω–∏–µ –≥—Ä–æ–º–∫–æ—Å—Ç–∏
            volume_factor = options.get('volume_factor', 1.0)
            if volume_factor != 1.0:
                try:
                    video = video.with_effects([afx.MultiplyVolume(volume_factor)])
                except:
                    # Fallback: —Ä—É—á–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –≥—Ä–æ–º–∫–æ—Å—Ç–∏
                    audio_array = video.audio.to_soundarray()
                    audio_array = audio_array * volume_factor
                    from moviepy import AudioArrayClip
                    new_audio = AudioArrayClip(audio_array, fps=video.audio.fps)
                    video = video.with_audio(new_audio)
            
            # –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏
            speed_factor = options.get('speed_factor', 1.0)
            if speed_factor != 1.0 and 0.5 <= speed_factor <= 2.0:
                try:
                    video = video.with_effects([vfx.MultiplySpeed(speed_factor)])
                except:
                    logger.warning(f"‚ö†Ô∏è –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –≤ —Ç–µ–∫—É—â–µ–π –≤–µ—Ä—Å–∏–∏ moviepy")
            
            # Fade in/out (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
            if options.get('audio_fadein', False) or options.get('audio_fadeout', False):
                try:
                    if options.get('audio_fadein', False):
                        video = video.with_effects([afx.AudioFadeIn(0.5)])
                    if options.get('audio_fadeout', False):
                        video = video.with_effects([afx.AudioFadeOut(0.5)])
                except:
                    logger.warning(f"‚ö†Ô∏è Audio fade —ç—Ñ—Ñ–µ–∫—Ç—ã –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è")
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞—É–¥–∏–æ: {e}")
        
        return video
    
    def _add_text_overlay(self, video: VideoFileClip, options: Dict) -> VideoFileClip:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –≤–∏–¥–µ–æ"""
        try:
            text_content = options.get('text_content', '')
            if not text_content:
                return video
            
            font_size = options.get('font_size', 50)
            font_color = options.get('font_color', 'white')
            position = options.get('text_position', 'bottom')
            duration = options.get('text_duration', min(video.duration, 5))
            
            txt_clip = TextClip(
                text_content,
                fontsize=font_size,
                color=font_color,
                font='Arial-Bold',
                method='caption',
                size=(video.w - 100, None)
            ).set_duration(duration)
            
            if position == 'top':
                txt_clip = txt_clip.set_position(('center', 50))
            elif position == 'center':
                txt_clip = txt_clip.set_position('center')
            else:  # bottom
                txt_clip = txt_clip.set_position(('center', video.h - 150))
            
            return CompositeVideoClip([video, txt_clip])
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞: {e}")
            return video
    
    def _optimize_video(self, video: VideoFileClip, options: Dict) -> VideoFileClip:
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–∏–¥–µ–æ –ø–æ —Ä–∞–∑–º–µ—Ä—É –∏ –∫–∞—á–µ—Å—Ç–≤—É"""
        quality = options.get('quality', self.default_quality)
        preset = self.quality_presets.get(quality, self.quality_presets['720p'])
        target_height = preset['height']
        
        if video.h != target_height:
            aspect_ratio = video.w / video.h
            target_width = int(target_height * aspect_ratio)
            
            # –û–∫—Ä—É–≥–ª—è–µ–º –¥–æ —á–µ—Ç–Ω—ã—Ö —á–∏—Å–µ–ª (—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ H.264)
            target_width += target_width % 2
            target_height += target_height % 2
            
            logger.info(f"üìê –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è: {video.w}x{video.h} ‚Üí {target_width}x{target_height}")
            
            try:
                # –ù–æ–≤–∞—è –≤–µ—Ä—Å–∏—è moviepy
                video = video.resized((target_width, target_height))
            except:
                try:
                    # –°—Ç–∞—Ä–∞—è –≤–µ—Ä—Å–∏—è moviepy
                    video = video.resize((target_width, target_height))
                except:
                    logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–º–µ–Ω–∏—Ç—å —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ")
        
        return video
    
    def _clear_file_metadata(self, file_path: Path):
        """–û—á–∏—Å—Ç–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–∞"""
        try:
            temp_path = file_path.with_suffix('.temp.mp4')
            
            cmd = [
                'ffmpeg', '-i', str(file_path),
                '-map_metadata', '-1',
                '-c', 'copy',
                str(temp_path),
                '-y', '-loglevel', 'error'
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                shutil.move(str(temp_path), str(file_path))
                logger.info("üîí –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ—á–∏—â–µ–Ω—ã")
            else:
                if temp_path.exists():
                    temp_path.unlink()
                    
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}")
    
    def _select_random_effects(self, count: int = None) -> List[str]:
        """–í—ã–±–æ—Ä —Å–ª—É—á–∞–π–Ω—ã—Ö —ç—Ñ—Ñ–µ–∫—Ç–æ–≤"""
        if count is None:
            count = random.randint(2, 5)
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ —ç—Ñ—Ñ–µ–∫—Ç–æ–≤ –¥–ª—è —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞
        color_effects = ['brightness', 'contrast', 'saturation', 'temperature']
        style_effects = ['vintage', 'cinematic', 'retro', 'neon']
        filter_effects = ['sharpen', 'film_grain', 'vignette']
        
        effects = []
        effects.extend(random.sample(color_effects, min(2, count)))
        if count > 2:
            effects.extend(random.sample(style_effects, min(1, count - 2)))
        if count > 3:
            effects.extend(random.sample(filter_effects, min(1, count - 3)))
        
        return effects[:count]
    
    def _calculate_uniqueness_score(self, effects: List[str], options: Dict) -> float:
        """–†–∞—Å—á–µ—Ç –∏–Ω–¥–µ–∫—Å–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏"""
        base_score = 0.5
        
        # –ë–æ–Ω—É—Å –∑–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç—Ñ—Ñ–µ–∫—Ç–æ–≤
        effects_bonus = min(len(effects) * 0.08, 0.3)
        
        # –ë–æ–Ω—É—Å –∑–∞ —Å–ª–æ–∂–Ω—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã
        complex_effects = {'glitch', 'fisheye', 'ascii', 'chromatic_aberration', 'cyberpunk'}
        complexity_bonus = len(set(effects) & complex_effects) * 0.05
        
        # –ë–æ–Ω—É—Å –∑–∞ –∫–∞—Å—Ç–æ–º–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        custom_bonus = 0.05 if options else 0
        
        total_score = base_score + effects_bonus + complexity_bonus + custom_bonus
        return min(total_score, 1.0)
    
    def _save_processing_info(self, result: ProcessingResult):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –æ–±—Ä–∞–±–æ—Ç–∫–µ"""
        try:
            info_file = Path(result.output_file).with_suffix('.json')
            with open(info_file, 'w', encoding='utf-8') as f:
                json.dump(result.to_dict(), f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏: {e}")
    
    def get_available_effects(self) -> Dict[str, List[str]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —ç—Ñ—Ñ–µ–∫—Ç–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º"""
        return {
            'color': ['brightness', 'contrast', 'saturation', 'hue', 'temperature'],
            'style': ['sepia', 'grayscale', 'vintage', 'cinematic', 'neon', 'retro', 'cyberpunk'],
            'filter': ['blur', 'sharpen', 'edge_enhance', 'emboss', 'noise', 'film_grain'],
            'geometry': ['vignette', 'fisheye', 'chromatic_aberration'],
            'special': ['glitch', 'pixelate', 'ascii', 'mirror_horizontal', 'mirror_vertical']
        }
    
    def cleanup_old_files(self, max_age_hours: int = 24):
        """–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        try:
            import time
            current_time = time.time()
            max_age_seconds = max_age_hours * 3600
            deleted_count = 0
            
            for directory in [self.output_dir, self.temp_dir]:
                for file_path in directory.iterdir():
                    if file_path.is_file():
                        file_age = current_time - file_path.stat().st_mtime
                        if file_age > max_age_seconds:
                            file_path.unlink()
                            deleted_count += 1
            
            logger.info(f"üßπ –û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –£–¥–∞–ª–µ–Ω–æ {deleted_count} —Ñ–∞–π–ª–æ–≤")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏: {e}")


# –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –¥–ª—è —É–¥–æ–±–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
__all__ = ['AdvancedVideoProcessor', 'ProcessingResult']
